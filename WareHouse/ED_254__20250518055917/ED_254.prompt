Define the function 'sum_of_indices(N: int, M: int, edges: List[Tuple[int, int]], Q: int, queries: List[Tuple[int, int]]) -> List[int]' to handle the input parameters. Parse the input to extract the number of vertices 'N', number of edges 'M', the list of edges 'edges', number of queries 'Q', and the list of queries 'queries'. Construct an adjacency list representation of the graph using the edges provided. Implement a function to perform a breadth-first search (BFS) or depth-first search (DFS) from a given vertex 'x_i' to find all vertices within distance 'k_i'. For each query '(x_i, k_i)', use the BFS/DFS function to determine the vertices within the specified distance. Calculate the sum of indices of the vertices found in the previous step for each query. Store the results of each query in a list to be returned. Ensure that the graph adheres to the constraints, particularly that the degree of each vertex does not exceed 3. Optimize the BFS/DFS to handle the maximum constraints efficiently, considering the graph's properties. Return the results as a list of integers corresponding to the sum of indices for each query.